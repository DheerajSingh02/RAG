{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U langchain-community\n",
        "# !pip install langchain_huggingface\n",
        "# ! pip install pypdf\n",
        "# !pip install chromadb"
      ],
      "metadata": {
        "id": "yDV5yKCpuZ2W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GTxaymmmt0of"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import tempfile\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Disable TensorFlow imports to avoid modeling_tf_utils error\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "\n",
        "# Set Hugging Face API token from Colab Secrets\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "PnSW2LZhyWYw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_document(pdf_path):\n",
        "    \"\"\"\n",
        "    Load and process PDF document with metadata\n",
        "    \"\"\"\n",
        "    # Load PDF\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Add metadata\n",
        "    for i, doc in enumerate(documents):\n",
        "        doc.metadata = {\n",
        "            \"source\": pdf_path,\n",
        "            \"page\": i + 1,\n",
        "            \"document_type\": \"pdf\"\n",
        "        }\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks\n",
        "\n",
        "def create_vector_store(chunks):\n",
        "    \"\"\"\n",
        "    Create and populate vector store using Hugging Face embeddings\n",
        "    \"\"\"\n",
        "    # Create a temporary directory with proper permissions\n",
        "    persist_directory = tempfile.mkdtemp()\n",
        "    print(f\"Creating Chroma DB at: {persist_directory}\")\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={'device': 'cpu'}  # Colab uses CPU by default\n",
        "    )\n",
        "\n",
        "    # Create new vector store with proper permissions\n",
        "    vector_store = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "\n",
        "    return vector_store, persist_directory\n",
        "\n",
        "def setup_conversational_chain(vector_store):\n",
        "    \"\"\"\n",
        "    Set up conversational retrieval chain with memory\n",
        "    \"\"\"\n",
        "    # Initialize Hugging Face LLM with correct task\n",
        "    llm = HuggingFaceEndpoint(\n",
        "        repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "        task=\"text-generation\",  # Specify the task explicitly\n",
        "        temperature=0.1,\n",
        "        max_new_tokens=512\n",
        "        # Uses os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] automatically\n",
        "    )\n",
        "\n",
        "    # Create memory for conversation history\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True\n",
        "    )\n",
        "\n",
        "    # Define prompt template\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"chat_history\", \"question\", \"context\"],\n",
        "        template=\"\"\"You are a knowledgeable assistant. Use the following context and chat history to answer the question concisely and accurately.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Chat History: {chat_history}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer: \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create conversational chain\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": 3}\n",
        "        ),\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": prompt_template}\n",
        "    )\n",
        "\n",
        "    return chain\n",
        "\n",
        "def setup_qa_system(chunks):\n",
        "    \"\"\"\n",
        "    Alternative approach without using persistent Chroma DB\n",
        "    \"\"\"\n",
        "    # Use HuggingFace embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={'device': 'cpu'}\n",
        "    )\n",
        "\n",
        "    # Create in-memory vector store\n",
        "    vector_store = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=None  # In-memory store\n",
        "    )\n",
        "\n",
        "    # Initialize LLM\n",
        "    llm = HuggingFaceEndpoint(\n",
        "        repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "        task=\"text-generation\",\n",
        "        temperature=0.1,\n",
        "        max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Create memory\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True\n",
        "    )\n",
        "\n",
        "    # Create prompt template\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"chat_history\", \"question\", \"context\"],\n",
        "        template=\"\"\"You are a knowledgeable assistant. Use the following context and chat history to answer the question concisely and accurately.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Chat History: {chat_history}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer: \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create chain\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": 3}\n",
        "        ),\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": prompt_template}\n",
        "    )\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "def main():\n",
        "    # Upload PDF in Colab\n",
        "    print(\"Please upload a PDF file:\")\n",
        "    uploaded = files.upload()\n",
        "    pdf_path = list(uploaded.keys())[0]  # Get the uploaded PDF's filename\n",
        "\n",
        "    # Process document\n",
        "    print(\"Processing document...\")\n",
        "    chunks = load_and_process_document(pdf_path)\n",
        "\n",
        "    # Set up QA system without persistence issues\n",
        "    print(\"Setting up QA system...\")\n",
        "    qa_chain = setup_qa_system(chunks)\n",
        "\n",
        "    # Example interaction loop\n",
        "    while True:\n",
        "        question = input(\"\\nAsk a question about the document (or 'quit' to exit): \")\n",
        "        if question.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Direct invocation of QA chain\n",
        "            response = qa_chain.invoke({\"question\": question})\n",
        "            print(\"\\nAnswer:\", response[\"answer\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            print(\"Try asking a different question or rephrase your query.\")\n"
      ],
      "metadata": {
        "id": "JEFqShzrxJeP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "c5p0SktvxQPn",
        "outputId": "ae0002e3-963a-45c6-f602-faea485a2616"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload a PDF file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7cc65901-2894-46cb-aa07-00bcc8e1c177\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7cc65901-2894-46cb-aa07-00bcc8e1c177\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving basics-of-data-science-kpk.pdf to basics-of-data-science-kpk (4).pdf\n",
            "Processing document...\n",
            "Setting up QA system...\n",
            "\n",
            "Ask a question about the document (or 'quit' to exit): what is data science?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Answer: \n",
            "\n",
            "Data science is a multidisciplinary field that combines mathematical, statistical, and computational methods to extract knowledge and insights from structured and unstructured data. It is used to analyze large amounts of data to extract meaningful insights for business decision-making. Data science can help detect fraud, prevent monetary losses, build intelligence in machines, enable better and faster decision-making, and recommend the right products to the right customers.\n",
            "\n",
            "Ask a question about the document (or 'quit' to exit): quit\n"
          ]
        }
      ]
    }
  ]
}